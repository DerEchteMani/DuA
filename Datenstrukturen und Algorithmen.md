# Data Structures & Algorithms I

- [Data Structures \& Algorithms I](#data-structures--algorithms-i)
  - [Asymptotische Schranken](#asymptotische-schranken)
  - [Elementare Datenstrukturen](#elementare-datenstrukturen)
    - [Lineares Feld (Array)](#lineares-feld-array)
    - [Lineare Liste (Linked List)](#lineare-liste-linked-list)
    - [Stapel (Stack)](#stapel-stack)
    - [Schlange (Queue)](#schlange-queue)
  - [Sortierverfahren](#sortierverfahren)
    - [Divide \& Conquer](#divide--conquer)
    - [Insertion Sort](#insertion-sort)
    - [Merge Sort](#merge-sort)
    - [Quick Sort](#quick-sort)
    - [Radix Sort](#radix-sort)
  - [Suchverfahren](#suchverfahren)
    - [Sequentielle Suche](#sequentielle-suche)
    - [Selbstanordnende Felder](#selbstanordnende-felder)
    - [Bin√§rsuche](#bin√§rsuche)
    - [Interpolationssuche](#interpolationssuche)
    - [Quadratische Bin√§rsuche](#quadratische-bin√§rsuche)
    - [Fast Search](#fast-search)
  - [Bin√§rb√§ume](#bin√§rb√§ume)
    - [Auslesereihenfolgen](#auslesereihenfolgen)
    - [sortierte Bin√§rb√§ume](#sortierte-bin√§rb√§ume)
    - [balancierte B√§ume](#balancierte-b√§ume)
    - [2-4 B√§ume](#2-4-b√§ume)
    - [B-Sort](#b-sort)
  - [Halden (Heaps)](#halden-heaps)
    - [heapify](#heapify)
    - [build\_heap](#build_heap)
    - [Heap Sort](#heap-sort)
  - [Hash Tables](#hash-tables)
    - [Hashing](#hashing)
    - [√úberl√§uferlisten](#√ºberl√§uferlisten)
    - [Offene Adressierung](#offene-adressierung)
  - [Dynamische Arrays](#dynamische-arrays)
  - [Amortisierte Analyse](#amortisierte-analyse)

---

## Asymptotische Schranken

- generelle Methode Wachstum von Funktion abzusch√§tzen
- O(1) < O(log n) < O(‚àön) < O(n) < O(n log n) < O(n^x) < O(x^n) < O(n!)
- O-Notation -> asymptotische obere Schranke
  - Definition: O(g(n)) = f(n) |¬†∆é c ‚àà R+, nùëú ‚àà N: f(n) <= c * g(n) ‚àÄ n >= nùëú
- Œ©-Notation -> asymptotisch untere Schranke
  - Definition: Œ©(g(n)) = f(n) |¬†∆é c ‚àà R+, nùëú ‚àà N: f(n) >= c * g(n) ‚àÄ n >= nùëú
- Œò-Notation -> asymptotisch exakte Schranke
  - Definition: Œò(g(n)) = f(n) |¬†∆é c1, c2 ‚àà R+, nùëú ‚àà N: c1 * g(n) <= f(n) <= c2 * g(n) ‚àÄ n >= nùëú
- Laufzeit best case:    T(n) = min{T(I): |I| = n}
- Laufzeit average case: T(n) = 1/(|I(n)|) ‚àëT(I) mit I(n) = {I: |I| = n}
- Laufzeit worst case:   T(n) = max{T(I): |I| = n}

## Elementare Datenstrukturen

|Operation            |Array|Linked List|Stack|Queue|
|---------------------|-----|-----------|-----|-----|
|Zugriff auf Element i|O(1) |O(i)       |     |     |
|Suchen               |O(n) |O(n)       |     |     |
|Einf√ºgen             |O(n) |O(1)       |O(1) |O(1) |
|Entfernen            |O(n) |O(1)       |O(1) |O(1) |

### Lineares Feld (Array)

- fixe Gr√∂√üe
- Zugriff auf Element A[i] -> O(1)
- Suchen -> O(n)
- Einf√ºgen -> O(n)
- L√∂schen -> O(n)

### Lineare Liste (Linked List)

- Zugriff auf Element A[i] -> O(i)
- Suchen -> O(n)
- Einf√ºgen -> O(1)
- L√∂schen -> O(1)
  - mit Suchen -> O(n)

### Stapel (Stack)

- LIFO
- Hinzuf√ºgen nur an letzte Stelle
- Nur Entfernen des letzten Elements
- alle Operationen -> O(1)

### Schlange (Queue)

- FIFO
- Hinzuf√ºgen nur an letzte Stelle
- Nur Entfernen des ersten Elements
- alle Operationen -> O(1)

## Sortierverfahren

- in-place
  - Zahlen im Inputfeld umgeordnet
  - Zu jedem Zeitpunkt nur eine konstante Anzahl davon au√üerhalb zwischengespeichert
- stabil
  - Elemente mit identischen Sortierschl√ºsseln in In- & Output in gleicher Reihenfolge
- adaptiv
  - vorsortierte Folgen werden effizienter sortiert
  - bessere Laufzeit f√ºr fast sortierete Folgen
- worst-case optimal
  - Jede Eingabe wird in O(n log n) Zeit sortiert
- worst-case Verhalten -> l√§ngster Ast in Entscheidungsbaum
- Idealer Algorithmus -> vollst√§ndiger Bin√§rbaum mit n! Bl√§ttern
- H√∂he Bin√§rbaum mit n! Bl√§ttern -> Œ©(n log n)

|Algorithmus  |in-place|stabil|adaptiv|best-case |avg-case  |worst-case|
|-------------|--------|------|-------|----------|----------|----------|
|InsertionSort|‚úì       |‚úì     |‚úì      |O(n)      |O(n^2)    |O(n^2)    |
|MergeSort    |x       |‚úì     |x      |O(n log n)|O(n log n)|O(n log n)|
|QuickSort    |‚úì       |x     |x      |O(n log n)|O(n log n)|O(n^2)    |
|RadixSort    |x       |‚úì     |x      |O(d * n)  |O(d * n)  |O(d * n)  |
|HeapSort     |‚úì       |x     |x      |O(n log n)|O(n log n)|O(n log n)|
|B-Sort       |x       |x     |‚úì      |O(n log n)|O(n log n)|O(n log n)|

### Divide & Conquer

- Teile -> Problem in Anzahl von Teilproblemen aufteilen
- Herrsche -> Teilprobleme durch rekursive Aufrufe l√∂sen
- Verbinde -> Teill√∂sungen zu Gesamtl√∂sung verbinden

### Insertion Sort

- best case:    Œ©(n)
- average case: O(n^2)
- worst case:   O(n^2)
- Speicher:     Œò(1)
- in-place
- stabil
- adaptiv

```
INSERTIONSORT(A, n)
    FOR i <- 1 TO n-1 DO
        h <- A[i]
        j <- i-1
        WHILE j >= 0 AND h < A[j] DO
            A[j+1] <- A[j]
            j <- j-1
        A[j+1] <- h
    RETURN A
```

### Merge Sort

- Teile Feld in 2 H√§lften
- H√§lften rekursiv sortieren
- sortierte Teilfolgen zu sortierter Gesamtfolge verschmelzen
- best case:    Œ©(n log n)
- average case: O(n log n)
- worst case:   O(n log n)
- Speicher:     Œò(n)
- stabil

```
MERGESORT(A, von, bis)
    IF von < bis THEN
        k <- floor((von+bis)/2)
        MERGESORT(A, von, k)
        MERGESORT(A, k+1, bis)
        VERSCHMELZE(A, von, k, bis)
```

### Quick Sort

- Zerlege Feld, dass Elemente links kleiner-gleich der Elemente rechts sind
- Zerlege Teilfelder rekursiv
- best case:    Œ©(n log n)
- average case: O(n log n)
- worst case:   O(n^2)
- Speicher:     O(n)
- in-place

```
QUICKSORT(A, von, bis)
    IF von < bis THEN
        k <- PARTITION(A, von, bis)
        QUICKSORT(A, von, k)
        QUICKSORT(A, k+1, bis)
```

### Radix Sort

- nach k Durchl√§ufen Zahlen, eingeschr√§nkt auf die letzten k Ziffern, sortiert
- nur bei gro√üen Datenmengen effizient
- hoher Speicheraufwand
- nicht vergleichsorientiert
- Streuphase
  - A nach i-ter Ziffer von hinten in F√§cher einordnen
- Sammelphase
  - F√§cher in aufsteigender Reihenfolge in A zusammenfassen

```
RADIXSORT(A, d)
    FOR i = 1 TO d
        {Streuphase}
        {Sammelphase}
```

## Suchverfahren

- ohne Vorsortierung
  - sequentielle Suche
  - selbstanordnende Felder
- mit Vorsortierung
  - Bin√§rsuche
  - Interpolationssuche
  - Quadratische Bin√§rsuche
  - Fastsearch
- Speicherbedarf
  - rekursive Algorithmen: proportional zur Laufzeit
  - iterative Algorithmen: O(1)

|Algorithmus|avg-case    |worst-case|
|-----------|------------|----------|
|SeqSearch  |Œò(n)        |Œò(n)      |
|BinSearch  |O(log n)    |O(log n)  |
|IntSearch  |O(log log n)|Œò(n)      |
|QuadSearch |O(log log n)|O(‚àön)     |
|FastSearch |O(log log n)|O(log n)  |

### Sequentielle Suche

- Feld von Anfang bis Ende durchsuchen
- average case: Œò(n)
- worst case:   Œò(n)
  - wenn jedes Element gleich oft gesucht
- Verbesserung durch Speichern der Elemente nach Zugriffswahrscheinlichkeit
  - Gleichverteilung:         O(n)
  - exponentielle Verteilung: O(1)

```
SEQSEARCH(A, x)
    i <- 0
    WHILE i < n DO
        IF A[i] = x THEN
            RETURN i
        i <- i+1
    RETURN NAN
```

### Selbstanordnende Felder

- Elemente, die h√§ufiger gesucht werden nach vorne verschieben
- 3 Heuristiken
  - vertausche A[i] mit A[0]
  - vertausche A[i] mit A[i-1]
  - Z√§hlen d. Zugriffe & dementsprechend umordnen

### Bin√§rsuche

- Feld in 2 gleich gro√üe H√§lften teilen
- Mit mittlerem Vergleichen
  - ident -> gefunden
  - ansonsten in linker (kleiner) / rechter (gr√∂√üer) H√§lfte wiederholen
- average case: O(log n)
- worst case:   O(log n)

```
BINSEARCH(A, x)
    IF len(A) > 0 THEN
        m <- floor(len(A)/2)
        IF x = A[m] THEN
            RETURN m
        ELSE
            IF y < A[m] THEN
                RETURN BINSEARCH(A[0...m-1], x)
            ELSE
                RETURN BINSEARCH(m+1+A[m+1...len(A)-1], x)
    ELSE
        RETURN NAN
```

### Interpolationssuche

- Suche nicht in der Mitte, sondern dort wo Element "sein sollte"
  - unter Annahme, dass Werte linear steigen
- average case: O(log log n)
- worst case:   Œò(n)

```
INTSEARCH(von, bis, x)
    IF A[von] < A[bis] THEN
        t <- von + floor((bis-von)*((x-A[von])/(A[bis]-A[von])))
        IF x = A[t] THEN
            RETURN t
        ELSE
            IF x < A[t] THEN
                RETURN INTSEARCH(von, t-1, x)
            ELSE
                RETURN INTSEARCH(t+1, bis, x)
    ELSE
        IF x = A[von] THEN
            RETURN von
        ELSE
            RETURN NAN
```

### Quadratische Bin√§rsuche

- Idee: verhindere worst-case d. Interpolationssuche durch Anwendung Interpolationssuche auf ‚àön Teilfelder der L√§nge vn
  - Berechne Index t durch Interpolationssuche
  - Suche von t aus korrektes Teilfeld (in Spr√ºngen von ‚àön) & suche von dort aus weiter
  - Identifizieren des korrekten Teilfelds in O(1) Zeit
- average case: O(log log n)
- worst case:   O(‚àön)

```
QUADSEARCH(von, bis, x)
    n <- bis-von+1
    IF A[von] < A[bis] THEN
        t <- von + floor((bis-von)*((x-A[von])/(A[bis]-A[von])))
        IF x = A[t] THEN
            RETURN t
        ELSE IF x < A[t] THEN
            REPEAT t <- t - floor(‚àön) UNTIL x >= A[t]
            RETURN QUADSEARCH(t, t + floor(‚àön)-1, x)
        ELSE
            REPEAT t <- t + floor(‚àön) UNTIL x <= A[t]
            RETURN QUADSEARCH(t - floor(‚àön)+1, t, x)
    ELSE IF x = A[von] THEN
        RETURN von
    ELSE
        RETURN NAN
```

### Fast Search

- Kombination Bin√§rsuche & Interpolationssuche
- average case: O(log log n)
- worst case:   O(log n)

```
FASTSEARCH(von, bis, x)
    IF von <= bis THEN
        m(B) <- floor((von+bis)/2)
        m(I) <- von + floor((bis-von)*((x-A[von])/(A[bis]-A[von])))
        IF m(B) > m(I) THEN
            VERTAUSCHE(m(B), m(I))
        IF x = A[m(B)] THEN 
            RETURN m(B)
        ELSE IF x = A[m(I)] THEN
            RETURN m(I)
        ELSE IF x < A[m(B)] THEN
            RETURN FASTSEARCH(von, m(B)-1, x)
        ELSE IF x < A[m(I)] THEN
            RETURN FASTSEARCH(m(B)+1, m(I)-1, x)
        ELSE
            RETURN FASTSEARCH(m(I)+1, bis, x)
    ELSE
        RETURN NAN
```

## Bin√§rb√§ume

- Wurzel -> ausgezeichneter Knoten ohne Parent (A[0])
- Blatt -> Knoten ohne Kinder
- H√∂he h -> max Anzahl der Anwendungen von parent um von beliebigen Knoten zur Wurzel zu gelangen
- Jeder Knoten hat genau 1 Parent
- Jeder Knoten ist Parent von maximal 2 Kindern
- Jeder Knoten ist Wurzel eines Teilbaumes
- Ordnung Knoten: Anzahl seiner Kinder
- Ordnung Baum: maximale Ordnung aller Knoten
- H√∂he Baum: L√§nge d. l√§ngsten Astes
- Voller Baum d. Ordnung K -> Jeder Knoten hat genau k Kinder oder ist ein Blatt
- Vollst√§ndiger Baum -> Voller Baum, bei dem jedes Blatt gleiche Tiefe hat
- grundlegende Operationen
  - Wurzel(B) -> Wurzelknoten d. Baumes
  - links(k) -> linkes Kind von k
  - rechts(k) -> rechtes Kind von k
  - Parent(k) -> Parentknoten von k
  - Wert(k) -> Wert in Knoten k

### Auslesereihenfolgen

- symmetrische Reihenfolge (SR)
  - in-order
  - linker Teilbaum in SR, Wurzel, rechter Teilbaum in SR
- Hauptreihenfolge (HR)
  - preorder
  - Wurzel, linker Teilbaum in HR, rechter Teilbaum in HR
- Nebenreihenfolge (NR)
  - postorder
  - linker Teilbaum in NR, rechter Teilbaum in NR, Wurzel

### sortierte Bin√§rb√§ume

- in symmetrischer Reihenfolge sortiert
  - Knoten linker Teilbaum <= Wurzel <= Knoten rechter Teilbaum
- Suchen: O(h)
  - h -> L√§nge d. l√§ngsten Astes
- Einf√ºgen: O(h)
- Minimum / Maximum: O(h)
- Aufbau
  - durch wiederholtes Einf√ºgen (nat√ºrliche B√§ume)
  - Bin√§rbaum h√§ngt von Reihenfolge d. Elemente ab
  - randomisiert Einf√ºgen -> E[h] = Œò(log n)
- Nachfolger von k 
  - n√§chster Knoten in sortierter Reihenfolge (n√§chstgr√∂√üerer / gleicher Wert in Baum)
  - Laufzeit: O(h)
- Vorg√§nger von k
  - Vorg√§ngerknoten in sortierter Reihenfolge
  - Laufzeit: O(h)
- Entfernen
  - a) k ist Blatt -> abh√§ngen
  - b) k hat nur 1 Kind -> Teilbaum des Kindes als Parent(k) anh√§ngen
  - c) k hat 2 Kinder
    - Finde k' -> n√§chster Knoten in sortierter Knotenfolge
    - Wert(k) = Wert(k')
    - Entferne k' (Fall a) oder b))
  - Laufzeit: O(h)
- Vorteil: dynamische L√∂sung d. W√∂rterbuchproblems
- Nachteil: Zeiten bis zu Œò(n) bei entarteten B√§umen

```
SEARCH(B, k)
    IF k = nil OR B = wert(k) THEN
        write k
    ELSE IF B < wert(k) then
        SEARCH(B, links(k))
    ELSE
        SEARCH(B, rechts(k))
```

```
INSERT(B, k)
    y <- nil
    x <- wurzel(B)
    WHILE x != nil DO
        y <- x
        IF wert(k) < wert(x) THEN
            x <- links(x)
        ELSE
            x <- rechts(x)
    parent(k) <- y
    IF y = nil THEN
        wurzel(B) <- k
    ELSE IF wert(k) < wert(y) THEN
        links(y) <- k
    ELSE rechts(y) <- k
```

### balancierte B√§ume

- Bedingungen zur Erhaltung logarithmischer Baumh√∂he:
  - H√∂henbedingung
    - h(B) -> H√∂he von B
    - F√ºr jeden Knoten gilt: |h(B(links)) - h(B(rechts))| <= k
  - Gewichtsbedingung
    - g(B) -> # Bl√§tter von B
    - F√ºr jeden Knoten gilt: 1/ùõº <= g(B(links))/g(B(rechts)) <= ùõº
  - Strukturbedingung
    - Alle Bl√§tter haben gleiche Tiefe, Ordnung d. Knoten (# Kinder) ist variabel

### 2-4 B√§ume

- Alle √Ñste gleich lang
- max # Kinder pro Knoten ist 4
- innere Knoten haben >= 2 Kinder
- Bl√§tter enthalten von links nach rechts Werte aufsteigend sortiert
- Jeder innere Knoten k mit ùõº(k) Kindern (2 <= ùõº(k) <= 4) speichert ùõº(k)-1 Hilfsinformationen
  - wobei x(i) = gr√∂√üter Wert im Teilbaum d. i-ten Kindes von links
- Innere Knoten speichern Hilfsinformationen
- H√∂he h: Œò(log n)
- Anzahl Bl√§tter wird mit jeder Schicht
  - mindestens verdoppelt
  - maximal vervierfacht
- Anzahl Knoten bei n Bl√§ttern
  - \# Knoten <= 2n = O(n)
- Suchen: Œò(log n)
- Einf√ºgen: O(log n)
  - ùõº(k) <= 4 -> resultierender Baum ist wieder 2-4 Baum
  - ùõº(k) = 5 -> Spalten von k
    - k an Bruder k' rechts von k geben
    - 2 rechtesten Kinder von k auf k' umh√§ngen 
    - (muss evtl. f√ºr √ºbergeordnete Knoten wiederholt werden)
- Entfernen: O(log n)
  - ùõº(k) >= 2 -> resultierender Baum ist wieder 2-4 Baum
  - ùõº(k) = 1 (k' -> direkter Bruder)
    - ùõº(k') >= 3 -> stehlen eines Kindes von k'
    - ùõº(k') = 2 -> verschmelzen von k mit k'
- Vorgenommene Umstrukturierungen amortisieren sich sp√§ter 

### B-Sort

- adaptiv & worst-case optimal
- Zahlenfolge verkehrt (a(n) ... a(1)) in 2-4 Baum einf√ºgen
- Innere Knoten speichert nur Maximum in Teilbaum
- Idee:
  - wenn f(i) klein, liegt w' tief
  - Bottom up einf√ºgen
    - starte bei linkestem Blatt
    - laufe bis zu w' (1. Knoten mit max > a(i))
    - F√ºge a(i) in Teilbaum ein
  - -> Anh√§ngen von a(i) in Œò(log f(i)) Zeit

## Halden (Heaps)

- A[i] >= max{A[2i+1], A[2i+2]}
- Zugriff auf Nachfolger / Vorg√§nger
  - links(i)  -> 2i+1
  - rechts(i) -> 2i+2
  - parent(i) -> floor((i-1)/2)
- A[0] -> Wurzel -> Maximum 
- Jeder Teilbaum wieder Halde
- H√∂he h -> floor(ld n)

### heapify

- Voraussetzung
  - Teilb√§ume mit Wurzel links(i) & rechts(i) sind Halden
  - Element i verletzt Haldenbedingung
- Laufzeit: O(log n)

```
HEAPIFY(A, i)
    l <- links(i)
    r <- rechts(i)
    index <- i
    IF l < len(A) AND A[l] > A[i] THEN
        index <- l
    IF r < len(A) AND A[r] > A[index] THEN
        index <- r
    IF i != index THEN
        VERTAUSCHE(A[i], A[index])
        HEAPIFY(A, index)
```

### build_heap

- Bl√§tter sind bereits triviale Halden
- Verhalde nur Elemente mit index i <= floor(n/2) - 1
- Verhalde bottom-up
- Laufzeit
  - Naive Analyse: O(n log n)
  - Aber: Element der H√∂he h kann in O(n) verhaldet werden: O(n)

```
BUILD_HEAP(A)
    FOR i <- floor(n/2) - 1 DOWNTO 0 DO
        HEAPIFY(A, i)
```

### Heap Sort

- Idee
  - Vertausche A[0], A[n-1]
  - Verkleinere Halde um 1
  - -> A[0] ist nicht mehr Maximum
  - -> Datenstruktur muss umstrukturiert werden
- best case:    O(n log n)
- average case: O(n log n)
- worst case:   O(n log n)
- in-place

```
HEAPSORT(A)
    BUILD_HEAP(A)
    FOR i <- n-1 DOWNTO 1 DO
        VERTAUSCHE(A[0], A[i])
        N <- N-1 // N ist aktuelle Haldengr√∂√üe
        HEAPIFY(A, 0)
```

## Hash Tables

- m -> Tabellengr√∂√üe
- n -> Anzahl eingef√ºgter Elemente
- ùõº -> Belegungsfaktor
  - n/m
  - ùõº > 1 -> Kollisionen unvermeidbar

### Hashing

- Anstatt zu suchen, berechne Datums-Adresse aus Schl√ºssel in O(1) Zeit
- gute Hashfunktion
  - h(w) soll effizient berechnet werden k√∂nnen
  - Jeder Index soll gleich wahrscheinlich sein, um Kollisionen zu verhindern
  - √Ñhnliche Werte sollen gut getrennt werden
  - h(w) soll unabh√§ngig von Mustern in den Daten sein
- ideale Hashfunktion (theoretisches Konstrukt)
  - Verwendung zur Analyse
  - existiert in der Praxis nicht
- reale Hashfunktionen
  - Divisionsmethode
    - Dividiere Wert durch m & nimm Rest
    - h(w) = w mod m
    - Vorteil: schnell berechenbar
    - Nachteil nicht f√ºr alle m gut
  - Multiplikationsmethode
    - Multipliziere Wert mit fixer Konstante A & multipliziere gebrochenen Teil mit m
    - h(w) = floor(m*frac(w*A))
    - Vorteil: m ist unkritisch

### √úberl√§uferlisten

- Bei Kollisionen Daten in verketteter Liste angelegt
- Einf√ºgen: O(1)
- Suchen: O(1+ùõº)
- L√∂schen: O(1+ùõº)
- worst case: Œò(1+n)
  - f√ºr Suchen & L√∂schen

### Offene Adressierung

- Bei Kollision wird so lange neue Adresse berechnet, bis freier Platz gefunden
- Ideale Hashfunktion -> f√ºr jeden Wert ist h(w) mit Wahrscheinlichkeit 1/m! eine der m! Permutationen
- In der Praxis verwendete N√§herungen
  - Linear Probing
    - h(w,i) = (h'(w)+1) mod m
    - Problem: benachbarte Felder wahrscheinlicher belegt (primary Clustering)
  - Quadratic Probing
    - h(w,i) = (h'(w)+f(i)) mod m
    - f(i) -> quadratische Funktion; bei Kollision weiterhin selbe Indexfolge (secondary Clustering)
  - Double Hashing
    - h(w,i) = ((h'(w)+ih''(w))) mod m
- Einf√ºgen: O(1/(1-ùõº))
- Suchen: O(1/(1-ùõº))
- L√∂schen: O(1/(1-ùõº))

## Dynamische Arrays

- dynamische Gr√∂√üe
- Zugriff auf Element i in konstanter Zeit
- Hinzuf√ºgen am Ende in (amortisierter) konstanter Zeit
- Entfernen am Ende in (amortisierter) konstanter Zeit
- dynamisches Array is Tupel DA = (A, n)
  - A -> Array d. Gr√∂√üe n(cap)
  - n -> Anzahl gespeicherter Elemente
- Grundprinzip
  - Array von links nach rechts auff√ºllen
  - mehr als n(cap) Elemente -> gr√∂√üe verdoppeln
  - Array wird von rechts nach links geschrumpft
  - l√∂schen & weniger als n(cap)/4 Elemente -> Gr√∂√üe halbieren
- Speicherverbrauch (n(cap)) -> Œò(n)
- worst case: Œ©(n)
  - f√ºr L√∂schen & Hinzuf√ºgen (wenn erweitert / geschrumpft wird)
- Amortisierte Laufzeit
  - k aufeinanderfolgende add / delete Operationen in beliebiger Reihenfolge
  - -> Laufzeit der k Operationen: O(k)

```
ADD(DA, x)
    IF n < n(cap) THEN
        A[n] <- x
        RETURN (A, n+1)
    ELSE
        A_old <- A
        A = new array of size 2n(cap)
        A[0,...,n-1] <- A_old[0,...,n-1]
        free(A_old)
        RETURN ADD((A, n), x)
```

```
DELETE(DA)
    IF n > n(cap)/4 THEN
        RETURN (A, n-1)
    ELSE
        A_old <- A
        A = new array of size n(cap)/2
        A[0,...,n-1] <- A_old[0,...,n-1]
        free(A_old)
        RETURN DELETE((A, n))
```

## Amortisierte Analyse

- zeigt, dass m aufeinanderfolgende (verschiedene) Operationen effizient sind
- Beispiel 2-4 Baum
  - einf√ºgen / entfernen ben√∂tigt nicht immer spalten / stehlen / ...
  - O(m log n) Umstrukturierungen nicht ben√∂tigt
  - m einf√ºgen / entfernen Operationen ben√∂tigen nur O(m) Umstrukturierungen
- Beispiel dynamisches Array
  - add / delete hat im worst case Œ©(n) Laufzeit
  - k add / delete Operationen braucht keine Œ©(k^2) Laufzeit
  - Laufzeit f√ºr k Operationen eigentlich O(k)
- Abrechnungstechnik
  - Datenstruktur bekommt Konto
  - Jede Operation auf Datenstruktur wird Ein- / Auszahlung zugeordnet
  - Auszahlungen entsprechen typischerweise Laufzeit f√ºr Operation
  - Durch Konto k√∂nnen Aufw√§nde in der Datenstruktur abgesch√§tzt werden
